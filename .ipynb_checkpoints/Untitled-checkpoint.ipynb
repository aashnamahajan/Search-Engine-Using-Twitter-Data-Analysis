{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloaded = drive.CreateFile({'id':\"13tNhU9cbcVyMvfhtgYZ8bGxSpO9sQJ8_\"}) \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "#downloaded.GetContentFile('usa_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import numpy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[]\n",
    "complete_tweets = []\n",
    "final_tweets = []\n",
    "brazil = [\"jairbolsonaro\", \"carlosbolsonaro\", \"cirogomes\", \"Francischini_\", \"GuilhermeBoulos\"]\n",
    "india = [\"ArvindKejriwal\", \"narendramodi\", \"PiyushGoyal\", \"sachinpilot\", \"sardesairajdeep\", \"ShashiTaroor\", \"VasundharaBJP\"]\n",
    "usa = [\"BernieSanders\",\"BetoORourke\",\"ewarren\",\"KamalaHarris\",\"tedlieu\",\"AdamSchefter\", \"CoryBooker\"]\n",
    "counter = 0\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/combined_brazil.json\",\"r\",encoding = \"utf-8\") as infile:\n",
    "  for line in infile:\n",
    "    counter += 1\n",
    "    try:\n",
    "      tweet = json.loads(line.rstrip('\\n'))\n",
    "    except:\n",
    "      break\n",
    "    print (counter)\n",
    "    if tweet[\"user\"][\"screen_name\"] in brazil and tweet[\"in_reply_to_status_id\"] == None:\n",
    "      if tweet[\"tweet_lang\"] != \"en\" and \"translation\" in tweet:\n",
    "        tweets.append(tweet[\"translation\"])\n",
    "        complete_tweets.append(tweet)\n",
    "      else:\n",
    "        tweets.append(tweet[\"text_en\"])\n",
    "        complete_tweets.append(tweet)\n",
    "    else:\n",
    "      final_tweets.append(tweet)\n",
    "df = pd.DataFrame({\"text\" : tweets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(complete_tweets)#-len(final_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "br_my_stop_words = text.ENGLISH_STOP_WORDS.union([\"10\",\"time\",\"brazil\",\"brazilian\",\"thank\",\"thanks\",\"don\",\"t\",\"got\",\"good\",\"like\",\"did\",\"make\",\"ll\",\"just\",\"yes\",\"thing\",\"sir\",\"help\",\"town\",\"day\",\"qb\",\"wr\",\"pm\",\"news\",\"https\",\"39\",\"pt\",\"let\", \"today\", \"bolsonaro\", \"quot\", \"watch\", \"video\", \"people\"])\n",
    "en_my_stop_words = text.ENGLISH_STOP_WORDS.union([\"thank\",\"thanks\",\"don\",\"t\",\"got\",\"good\",\"like\",\"did\",\"make\",\"ll\",\"just\",\"yes\",\"thing\",\"sir\",\"help\",\"town\",\"day\",\"qb\",\"wr\",\"people\",\"rb\",\"hc\"])\n",
    "#hi_my_stop_words = text.ENGLISH_STOP_WORDS.union([\"prime\",\"time\",\"indian\",\"shri\",\"ji\",\"india\",\"thank\",\"thanks\",\"don\",\"t\",\"got\",\"good\",\"like\",\"did\",\"make\",\"ll\",\"just\",\"yes\",\"thing\",\"sir\",\"pm\",\"https\",\"god\",\"tv\",\"watch\",\"today\",\"39\",\"life\"])\n",
    "#hi_my_stop_words = text.ENGLISH_STOP_WORDS.union([\"don\",\"pm\",\"news\",\"day\",\"https\",\"ji\",\"shri\",\"years\",\"today\",\"39\",\"new\",\"time\",\"god\",\"tv\"])\n",
    "hindi_stopwords = text.ENGLISH_STOP_WORDS.union([\"don\",\"t\",\"got\",\"good\",\"like\",\"did\",\"make\",\"ll\",\"just\",\"yes\",\"thing\",\"sir\",\"ji\",\"today\",\"39\",\"mr\",\"pm\",\"tv\",\"shri\",\"wonderful\",\"thank\",\"wishes\",\"day\",\"met\",\"various\",\"time\",\"years\",\"going\"])\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words=br_my_stop_words)\n",
    "doc_term_matrix = count_vect.fit_transform(df['text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (LDA.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_india = {0:\"governemnt\", 1: \"politics\", 2:\"government\", 3:\"development work\", 4:\"bjp\", 5:\"birthday wishes\", 6:\"indian railways\"}\n",
    "dict_brazil = {0:\"ciro\", 1:\"lula\", 2:\"government crisis\", 3:\"education\", 4:\"pdt\"}\n",
    "i = 0\n",
    "\n",
    "for tweet in complete_tweets:\n",
    "  tweet[\"topic\"] = dict_brazil[int(topic_values[i].argmax())]\n",
    "  i += 1\n",
    "  for twt in final_tweets:\n",
    "    if tweet[\"id\"] == twt[\"in_reply_to_status_id\"]:\n",
    "        twt[\"topic\"] = tweet[\"topic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_tweets.extend(final_tweets) \n",
    "\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/combined_brazil_topics.json\",\"w\",encoding = \"utf-8\") as f:\n",
    "  for tweet in complete_tweets:\n",
    "    json.dump(tweet, f, ensure_ascii=False)\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_tweets[30667]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in complete_tweets:\n",
    "    if tweet[\"id\"] == 1170148460705669120:\n",
    "    print (tweet[\"text_en\"])\n",
    "    print (tweet[\"topic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(complete_tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
